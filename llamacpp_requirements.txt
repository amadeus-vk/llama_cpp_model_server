litellm[proxy]>=1.35.0
llama-cpp-python