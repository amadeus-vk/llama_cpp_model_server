litellm[proxy]
llama-cpp-python