version: '3.8'

services:
  llamacpp-server:
    build:
      context: .
      dockerfile: llamacpp.Dockerfile
    image: llamacpp-server-final # Give it a new name
    # Use the image you built with the script
    
    container_name: llamacpp-vulkan-cblast
    # Equivalent to -it from the 'docker run' command
    stdin_open: true
    tty: true
    
    # Equivalent to --device=/dev/dri:/dev/dri
    # This gives the container access to your GPU (for Intel/AMD)
    devices:
      - "/dev/dri:/dev/dri"
      
    # Equivalent to -v "$MODEL_HOST_DIR":/models:ro
    # Mounts your model directory as read-only
    volumes:
      - "/media/data/gguf-models:/models:ro"
      
    # Equivalent to -p $HOST_PORT:8080
    # Maps port 4000 on your host to port 8080 in the container
    ports:
      - "4000:8080"
      
    # The Dockerfile sets the final WORKDIR, but being explicit is good.
    # working_dir: /app/llama.cpp/build/bin
    
    # This is the command that runs inside the container
    # It uses the specific model and settings from your script
    command: [
      "sh", 
      "-c",
      "python3 -m llama_cpp.server --model /models/Phi-3-mini-4k-instruct-q4.gguf --n-gpu-layers 999 --host 0.0.0.0 --port 8080"
    ]
    networks:
      - agent_network
      - certifiied-proxy_proxy-network
  
networks:
  agent_network:
    external: true
  certifiied-proxy_proxy-network:
    external: true
