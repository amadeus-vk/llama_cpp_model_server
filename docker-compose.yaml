version: '3.8'

services:
  llamacpp-server:
    build:
      context: .
      dockerfile: llamacpp.Dockerfile
    image: llamacpp-server-final # Give it a new name
    # Use the image you built with the script
    
    container_name: llamacpp-vulkan-cblast
    # Equivalent to -it from the 'docker run' command
    stdin_open: true
    tty: true
    
    # Equivalent to --device=/dev/dri:/dev/dri
    # This gives the container access to your GPU (for Intel/AMD)
    devices:
      - "/dev/dri:/dev/dri"
      
    # Equivalent to -v "$MODEL_HOST_DIR":/models:ro
    # Mounts your model directory as read-only
    volumes:
      - "/media/data/gguf-models:/models:ro"
      
    # Equivalent to -p $HOST_PORT:8080
    # Maps port 4000 on your host to port 8080 in the container
    ports:
      - "4000:8080"
      
    environment:
      # Llama.cpp Command Arguments
      - MODEL=/models/Phi-3-mini-4k-instruct-q4.gguf
      - N_GPU_LAYERS=999
      - PORT=8080
      - HOST=0.0.0.0
      
      # ⬇️ VULKAN RUNTIME ENVIRONMENT VARIABLES ⬇️
      # These variables must match the paths from your Dockerfile's compilation phase
      - VULKAN_SDK_PATH=/app/vulkansdk*
      - LD_LIBRARY_PATH=$VULKAN_SDK_PATH/lib:$LD_LIBRARY_PATH
      - VK_LAYER_PATH=$VULKAN_SDK_PATH/etc/vulkan/explicit_layer.d
      
    command: [
      "sh", 
      "-c",
      # ... (your command remains the same)
      "/usr/bin/python3.11 -m llama_cpp.server --model $MODEL --n_gpu_layers $N_GPU_LAYERS --host $HOST --port $PORT"
    ]    
    # The Dockerfile sets the final WORKDIR, but being explicit is good.
    # working_dir: /app/llama.cpp/build/bin
    
    # This is the command that runs inside the container
    # It uses the specific model and settings from your script
    
    networks:
      - agent_network
      - certifiied-proxy_proxy-network
  
networks:
  agent_network:
    external: true
  certifiied-proxy_proxy-network:
    external: true
