version: '4.4'
llamacpp-server:
    build:
      context: .
      dockerfile: llamacpp.Dockerfile
    image: llamacpp-server-final # Give it a new name
    container_name: nollama
    restart: unless-stopped
    ports:
      - "4000:4000"
    volumes:
      - /media/data/gguf_models:/models:ro
    devices:
      - /dev/dri:/dev/dri
    environment:
      - MODEL=/models/qwen2-7b-instruct-q4_k_m.gguf # Which model to load
      - N_GPU_LAYERS=-1                              # Offload all layers to GPU
      - PORT=4000                                    # Port inside the container
      - HOST=0.0.0.0                                 # Host inside the container
    command: >
      sh -c "python3 -m llama_cpp.server"