version: '3.9'

services:
  llamacpp-server:
    build:
      context: .
      dockerfile: llamacpp.Dockerfile
    image: llamacpp-server-final # Give it a new name
    # Use the image you built with the script
    
    container_name: llamacpp-vulkan-cblast
    # Equivalent to -it from the 'docker run' command
    stdin_open: true
    tty: true
    
    # Equivalent to --device=/dev/dri:/dev/dri
    # This gives the container access to your GPU (for Intel/AMD)
    devices:
      - "/dev/dri:/dev/dri"
      
    # Equivalent to -v "$MODEL_HOST_DIR":/models:ro
    # Mounts your model directory as read-only
    volumes:
      - "/media/data/gguf-models:/models:ro"
      
    # Equivalent to -p $HOST_PORT:8080
    # Maps port 4000 on your host to port 8080 in the container
    ports:
      - "4000:8080"
      
    environment:
      MODEL: "/models/Phi-3-mini-4k-instruct-q4.gguf"
      N_GPU_LAYERS: 999
      PORT: 8080
      HOST: "0.0.0.0"
      VULKAN_SDK_PATH: "/app/1.3.283.0/x86_64"
      LD_LIBRARY_PATH: "/app/1.3.283.0/x86_64/lib:/lib"
      VK_LAYER_PATH: "/app/1.3.283.0/x86_64/etc/vulkan/explicit_layer.d"
      
    command:
      - "/usr/bin/python3"
      - "-m"
      - "llama_cpp.server"
      - "--model"
      - "/models/Phi-3-mini-4k-instruct-q4.gguf"
      - "--n_gpu_layers"
      - "-1"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
    # The Dockerfile sets the final WORKDIR, but being explicit is good.
    # working_dir: /app/llama.cpp/build/bin
    
    # This is the command that runs inside the container
    # It uses the specific model and settings from your script
    
    networks:
      - agent_network
      - certifiied-proxy_proxy-network
  
networks:
  agent_network:
    external: true
  certifiied-proxy_proxy-network:
    external: true