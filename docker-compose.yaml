version: '3.8'

services:
 
  llamacpp-server:
    # This service will be built using 'llamacpp.Dockerfile'
    build:
      context: .
      dockerfile: llamacpp.Dockerfile
    # The 'image' tag gives the locally built image a name
    image: llama-cpp-model-server
    container_name: nollama
    restart: unless-stopped
    volumes:
      - /media/data/docker/volumes/ollama/_data/models:/models:ro
    ports:
      - "4000:4000"
    devices:
      - /dev/dri:/dev/dri
    networks:
      - agent_network
   
networks:
  agent_network:
    external: true
